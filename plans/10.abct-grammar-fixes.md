# Plan: ABCT-Specific Fixes for feature/abct-grammar Branch

## Table of Contents

1. [Overview](#overview)
2. [Issues to Address](#issues-to-address)
3. [Technical Design](#technical-design)
4. [Implementation Plan with Integrated Testing](#implementation-plan-with-integrated-testing)
5. [Verification](#verification)

---

## Overview

Worktree: `/workspace/worktrees/abct-grammar-fixes`
Branch: `feature/abct-grammar`

This plan addresses three ABCT-specific issues using the same programming patterns as the ABC scanner/parser: function-based design with context objects, not class-based OOP.

---

## Issues to Address

### Issue 1: Parser Error Recovery (272fc93b)

Problem: The Peggy parser crashes entirely on incomplete expressions, breaking syntax highlighting while typing.

Decision: Replace Peggy with a hand-written recursive descent parser following the ABC scanner/parser style. This gives full control over error recovery.

### Issue 2: Parentheses Preservation (21267b90)

Problem: The Peggy grammar discards parentheses, losing user intent for precedence grouping.

Decision: Add a `GroupExpr` wrapper node to the AST.

### Issue 3: Formatter Line Breaks (c40682d3)

Problem: The formatter collapses multi-line pipe expressions to single lines.

Decision: Preserve EOL tokens in the AST so the formatter can check where user line breaks were and preserve them.

### Removed: Selector Output (b39b5ec1)

This comment was written in error. Selectors mark nodes for downstream processing; they do not filter output. Current behavior is correct.

---

## Technical Design

### 1. Programming Style: Function-Based with Context Objects

Following the ABC scanner/parser pattern from `parse/parsers/scan2.ts`:

Scanner Context:
```
interface AbctCtx
  source: string
  tokens: Token[]
  start: number
  current: number
  line: number
  abcContext: ABCContext  # holds error reporter, options

  test(pattern): boolean   # check pattern at current position
  push(tokenType): void    # push token from start to current
  report(message): void    # report error via abcContext.errorReporter
```

Parser Context:
```
interface AbctParseCtx
  tokens: Token[]
  current: number
  abcContext: ABCContext
  errors: ParseError[]

  peek(): Token
  advance(): Token
  match(type): boolean
  check(type): boolean
  isAtEnd(): boolean
```

All scanner/parser functions are exported standalone functions that accept the context and return booleans. This enables the composition pattern shown below.

Example from ABC scanner demonstrating the pattern:
```
export function scanTune(ctx: Ctx): boolean {
  if (!tuneStartBeforeSectBrk(ctx.source.substring(ctx.current))) return false;
  while (!isAtEnd(ctx) && !ctx.test(pSectionBrk)) {
    ctx.start = ctx.current;
    // Try each tokenizer function in order of precedence
    if (scanDirective(ctx)) continue;
    if (comment(ctx)) continue;
    if (info_line(ctx)) continue;
    if (tuplet(ctx)) continue;
    if (slur(ctx)) continue;
    if (chord(ctx)) continue;
    if (barline2(ctx)) continue;
    if (note(ctx)) continue;
    if (rest(ctx)) continue;
    if (WS(ctx)) continue;
    if (EOL(ctx)) continue;
    // If no match is found, collect invalid characters into a token
    collectInvalidToken(ctx);
  }
  return true;
}

export function slur(ctx: Ctx): boolean {
  if (!ctx.test(/[()]/)) return false;
  advance(ctx);
  ctx.push(TT.SLUR);
  return true;
}
```

Key points:
- Sub-functions return `boolean`: `true` if matched and consumed, `false` if not
- Composition via `if (fn(ctx)) continue;` tries functions in precedence order
- Each function tests, advances cursor, pushes token(s), returns `true`
- If nothing matches, error recovery collects invalid tokens

ABCT scanner functions follow this same pattern:
```
export function scanProgram(ctx: AbctCtx): Token[]  # entry point, returns ctx.tokens
export function scanStatement(ctx: AbctCtx): boolean
export function scanIdentifier(ctx: AbctCtx): boolean
export function scanNumber(ctx: AbctCtx): boolean
export function scanOperator(ctx: AbctCtx): boolean
# etc.
```

Parser functions:
```
export function parseProgram(ctx: AbctParseCtx): Program
export function parseStatement(ctx: AbctParseCtx): Statement
export function parsePipeline(ctx: AbctParseCtx): Expr
export function parseAtom(ctx: AbctParseCtx): Expr
```

### 2. Token Types

```
enum AbctTT
  # Literals
  IDENTIFIER
  NUMBER
  STRING
  ABC_LITERAL   # <<...>>

  # Operators
  PIPE          # |
  PIPE_EQ       # |=
  PLUS          # +
  EQ            # =
  AT            # @
  COLON         # :
  MINUS         # -
  LPAREN        # (
  RPAREN        # )
  LBRACKET      # [
  RBRACKET      # ]
  LT_LT         # <<
  GT_GT         # >>

  # Comparison
  GT, LT, GTE, LTE, EQEQ, BANGEQ

  # Keywords
  AND, OR, NOT

  # Whitespace (preserved in AST)
  WS            # horizontal whitespace
  EOL           # end of line
  COMMENT       # # to end of line

  # Special
  EOF
  INVALID       # error recovery token
```

### 3. AST Node Types

Add to `abct/src/ast.ts`:

```
interface GroupExpr
  type: "group"
  expr: Expr
  openParen: Token   # ( token with location
  closeParen: Token  # ) token with location
  loc: SourceLocation

interface ErrorExpr
  type: "error"
  message: string
  partial?: Expr     # partial AST if available
  loc: SourceLocation
```

Existing Pipe, Concat, etc. nodes remain unchanged - no generic BinaryExpr needed. The existing left-associative structure handles chained operators: `a | b | c` becomes `Pipe(Pipe(a, b), c)`.

### 4. EOL Token Preservation: Interleaved Stream Model

Statements contain an interleaved stream of tokens and expressions. WS, EOL, and COMMENT tokens are first-class elements in the stream.

```
interface Statement
  type: "statement"
  elements: Array<Token | Expr>  # interleaved tokens and expressions
```

Example:
```
# Input:
transpose 2
| retrograde

# Token stream from scanner:
[IDENTIFIER "transpose", WS " ", NUMBER "2", EOL "\n", PIPE "|", WS " ", IDENTIFIER "retrograde", EOF]

# AST (Statement.elements):
[
  Application([Identifier(transpose), Number(2)]),
  Token(EOL, "\n"),
  Token(PIPE, "|"),
  Token(WS, " "),
  Identifier(retrograde)
]
```

The formatter walks the elements array linearly. When it encounters EOL tokens before a PIPE, it preserves the line break in output.

Handling blank lines (canonical approach from Roslyn, Prettier, go fmt):
- Blank lines (consecutive EOLs) are treated as leading tokens of the following statement
- This associates whitespace with "what comes next" conceptually
- The formatter may collapse multiple blank lines to at most one in output

```
# Input:
x = transpose 2

y = retrograde

# AST:
Program
  statements: [
    Statement(elements: [Assignment(x, Application(...))]),
    Statement(elements: [Token(EOL), Token(EOL), Assignment(y, ...)])  # blank line is leading
  ]
```

### 5. Error Recovery Strategy

Recovery points:
1. EOL (statement boundary)
2. `=` at statement level (assignment start)
3. `|` at expression level (pipe boundary)
4. Closing delimiters: `)`, `]`, `>>`

Synchronize function:
```
function synchronize(ctx: AbctParseCtx): void
  advance(ctx)
  while not isAtEnd(ctx):
    # Recovered at statement boundary
    if previous(ctx).type == EOL:
      return

    # Recovered at known statement start
    switch peek(ctx).type:
      case EQ:     # assignment
      case PIPE:   # pipe (at top level)
        return

    advance(ctx)
```

When an error is encountered:
1. Record error with location
2. Create ErrorExpr with partial AST if available
3. Call synchronize() to skip to recovery point
4. Continue parsing

### 6. Formatter Line Break Algorithm

The formatter walks the Statement.elements array linearly. When it encounters EOL tokens, it preserves them in output:

```
function formatStatement(stmt: Statement): string
  output = []
  for element in stmt.elements:
    if isToken(element):
      if element.type == EOL:
        output.push("\n")
      elif element.type == WS:
        output.push(" ")  # normalize to single space
      elif element.type == PIPE:
        output.push("| ")
      # ... other tokens
    else:
      output.push(formatExpr(element))
  return output.join("")
```

For auto-breaking long expressions (when user didn't add line breaks):
```
function shouldAutoBreak(elements: Array<Token | Expr>): boolean
  # Count pipes, estimate total length
  pipeCount = elements.filter(e => isToken(e) && e.type == PIPE).length
  hasUserBreaks = elements.some(e => isToken(e) && e.type == EOL)

  if hasUserBreaks:
    return false  # respect user formatting

  estimatedLength = estimateLength(elements)
  return estimatedLength > MAX_LINE_LENGTH or pipeCount >= PIPE_THRESHOLD
```

Configuration defaults:
- MAX_LINE_LENGTH: 80
- PIPE_THRESHOLD: 3

---

## Implementation Plan with Integrated Testing

Each phase includes both implementation and testing. Tests are written alongside code, not after.

Before starting implementation:
1. Copy this plan file to `/workspace/abc_parse/plans/10.abct-grammar-fixes.md`

Workflow for each phase:
1. Implement the phase (code + tests)
2. Run `npm run test` to ensure all tests pass
3. Call the code-review agent on the phase's changes
4. Address any feedback from the code review
5. Run tests again after addressing feedback
6. Commit the changes
7. Move to the next phase

Repeat until all phases are complete.

### Phase 1: Scanner Infrastructure

Files to create:
- `abct/src/scanner/types.ts` - Token types enum, Token class
- `abct/src/scanner/context.ts` - AbctCtx class
- `abct/src/scanner/utils.ts` - advance, peek, isAtEnd, consume helpers

No separate tests for infrastructure - these will be implicitly tested by scanner function tests in Phase 2.

### Phase 2: Basic Scanner Functions

Files to create:
- `abct/src/scanner/primitives.ts` - identifier, number, string, operators
- `abct/src/scanner/whitespace.ts` - WS, EOL, comment

Generator file:
- `abct/tests/scanner/generators.ts` - Token generators for PBT
  - Reuse existing generators from current ABCT implementation where possible (check `abct/tests/` for existing generators)

Tests for each function:
```
# Example-based
it("should scan identifier 'transpose'", () => {
  const ctx = createCtx("transpose")
  const result = identifier(ctx)
  expect(result).to.be.true
  expect(ctx.tokens[0].type).to.equal(AbctTT.IDENTIFIER)
  expect(ctx.tokens[0].lexeme).to.equal("transpose")
})

# Property-based
it("property: valid identifiers scan correctly", () => {
  fc.assert(fc.property(
    fc.stringMatching(/^[a-zA-Z_][a-zA-Z0-9_]{0,20}$/),
    (id) => {
      const ctx = createCtx(id)
      const result = identifier(ctx)
      return result && ctx.tokens[0].lexeme === id
    }
  ))
})
```

### Phase 3: Complete Scanner

Files:
- `abct/src/scanner/scanner.ts` - Main scan() function composing primitives
- `abct/src/scanner/index.ts` - Public API

Round-trip generator:
- `abct/tests/scanner/generators.ts` - Add genProgram generator

Tests:
```
# Round-trip test
it("property: scan produces tokens that reconstruct source", () => {
  fc.assert(fc.property(
    genValidAbctSource,
    (source) => {
      const ctx = createCtx(source)
      scan(ctx)
      const reconstructed = ctx.tokens.map(t => t.lexeme).join("")
      return reconstructed === source
    }
  ))
})
```

### Phase 4: AST Updates

Files:
- `abct/src/ast.ts` - Add GroupExpr, ErrorExpr, update Expr union

Tests:
- `abct/tests/ast.spec.ts` - Type guard tests, node creation tests

### Phase 5: Parser Infrastructure

Files:
- `abct/src/parser/context.ts` - AbctParseCtx class
- `abct/src/parser/utils.ts` - peek, advance, match, check, consume helpers
- `abct/src/parser/recovery.ts` - synchronize, error handling

Tests:
- `abct/tests/parser/context.spec.ts` - Context navigation tests
- `abct/tests/parser/recovery.spec.ts` - Synchronization tests

### Phase 6: Parser Expression Functions

Files:
- `abct/src/parser/atoms.ts` - parseIdentifier, parseNumber, parseGroup, parseList
- `abct/src/parser/expressions.ts` - parsePipeline, parseConcat, parseUpdate, parseApplication

Generator file:
- `abct/tests/parser/generators.ts` - Expression generators returning { tokens, expr }
  - Reuse existing ABCT generators where possible

For each parser function, write:
1. Example-based tests with concrete token arrays
2. Property-based tests using generators
3. Round-trip tests: tokens -> parse -> format -> compare

```
# Example-based
it("should parse grouped expression", () => {
  const tokens = [
    createToken(AbctTT.LPAREN, "("),
    createToken(AbctTT.IDENTIFIER, "transpose"),
    createToken(AbctTT.NUMBER, "2"),
    createToken(AbctTT.RPAREN, ")"),
  ]
  const ctx = createParseCtx(tokens)
  const result = parseAtom(ctx)
  expect(isGroup(result)).to.be.true
  expect(isApplication(result.expr)).to.be.true
})

# Round-trip
it("property: parse then format preserves semantics", () => {
  fc.assert(fc.property(
    genPipeExpr,
    ({ tokens, expr }) => {
      const ctx = createParseCtx(tokens)
      const parsed = parsePipeline(ctx)
      const formatted = format(parsed)
      const reparsed = parse(formatted)
      return astEquals(parsed, reparsed)
    }
  ))
})
```

### Phase 7: Error Recovery Integration

Files:
- `abct/src/parser/parser.ts` - Main parse() with error recovery

Tests:
- `abct/tests/parser/recovery.spec.ts` - Incomplete expression tests

```
it("should recover from incomplete pipe", () => {
  const tokens = scan("transpose 2 |")
  const result = parse(tokens)
  expect(result.errors).to.have.length(1)
  expect(result.ast.statements).to.have.length(1)
  # First statement should have partial AST
})

it("should parse valid statements after error", () => {
  const tokens = scan("x = |\ny = transpose 2")
  const result = parse(tokens)
  expect(result.errors).to.have.length(1)
  expect(result.ast.statements).to.have.length(2)
  # Second statement should be valid
})
```

### Phase 8: Formatter Updates

Files:
- `abc-lsp-server/src/abct/AbctFormatter.ts` - Update for GroupExpr, line breaks

Tests:
- `abc-lsp-server/tests/abct/formatter.spec.ts` - Formatting tests

```
# Parentheses preservation
it("should preserve parentheses in output", () => {
  const input = "(transpose 2 | retrograde)"
  const ast = parse(scan(input))
  const output = format(ast)
  expect(output).to.equal("(transpose 2 | retrograde)")
})

# Line break preservation
it("should preserve user line breaks", () => {
  const input = "transpose 2\n| retrograde"
  const ast = parse(scan(input))
  const output = format(ast)
  expect(output).to.include("\n|")
})

# Property: idempotence
it("property: formatting is idempotent", () => {
  fc.assert(fc.property(
    genValidProgram,
    (source) => {
      const ast = parse(scan(source))
      const f1 = format(ast)
      const f2 = format(parse(scan(f1)))
      return f1 === f2
    }
  ))
})
```

### Phase 9: Integration

Files:
- `abct/src/parser/index.ts` - Public API (same interface as current)
- `abc-lsp-server/src/AbctDocument.ts` - Use new parser

Final cleanup:
- Delete `abct/src/grammar.peggy`
- Delete `abct/src/parser.generated.d.ts`
- Update build scripts

No additional tests needed - the round-trip and PBT tests from earlier phases provide sufficient coverage.

---

## Verification

All verification is automated. No manual testing.

Run full test suite:
```bash
cd /workspace/worktrees/abct-grammar-fixes
npm run test
```

Test coverage must include:
- Every scanner function: example + PBT
- Every parser function: example + PBT + round-trip
- Error recovery: specific incomplete expression cases
- Formatter: parentheses, line breaks, idempotence

The round-trip property tests are the primary validation - if parse -> format -> parse produces equivalent ASTs, the implementation is correct.
