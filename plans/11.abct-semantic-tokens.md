# Semantic Token Highlighting for ABCT Language

## Table of Contents

1. [Overview](#overview)
2. [Design Decision](#design-decision)
3. [Token Type Mapping](#token-type-mapping)
4. [Implementation Steps](#implementation-steps)
5. [Files to Create](#files-to-create)
6. [Files to Modify](#files-to-modify)
7. [Verification](#verification)

---

## Overview

Add semantic token highlighting to the ABCT language in the abc_parse language server. ABCT files will use the `.abct` extension.

The work will be done in a new worktree branch based on `feature/abct-grammar`.

---

## Design Decision

We will reuse the existing Peggy parser rather than creating a separate scanner. The approach:

1. Modify the Peggy grammar to capture location info using Peggy's built-in `location()` function
2. Update AST types to include a `loc` property on each node
3. Create a visitor that walks the AST and produces a flat list of positioned tokens

This approach:
- Reuses the existing parser without duplication
- Keeps a single source of truth for the language structure
- Location info is useful for diagnostics beyond just highlighting

---

## Token Type Mapping

| ABCT Construct | Semantic Scope |
|----------------|----------------|
| `#` comments | comment |
| `and`, `or`, `not` | keyword |
| `\|`, `\|=`, `+`, `=` | operator |
| `>=`, `<=`, `==`, `!=`, `>`, `<` | operator |
| `<<...>>` ABC literals | string |
| Numbers (including fractions) | number |
| Identifiers (functions/transforms) | function |
| Identifiers (assigned variables) | variable |
| File paths | string |
| Selectors (`@chords`, `@V:melody`) | decorator |
| `@` symbol | decorator |
| Location selectors (`:10:5`) | number |
| Voice refs (`V:soprano`) | type |

---

## Implementation Steps

### Step 0: Create Worktree Branch

```
cd /workspace/abc_parse
git worktree add ../worktrees/abct-semantic-tokens -b feature/abct-semantic-tokens feature/abct-grammar
```

### Step 1: Add Location Interface to AST Types

Update `abct/src/ast.ts` to add a `Loc` interface and include it in all AST nodes:

```
interface Loc {
  start: { line: number; column: number; offset: number };
  end: { line: number; column: number; offset: number };
}

// Add loc property to all node interfaces
interface Identifier {
  type: "identifier";
  name: string;
  loc: Loc;
}
// ... same for all other node types
```

### Step 2: Modify Peggy Grammar to Capture Locations

Update `abct/src/grammar.peggy` to call `location()` in each rule:

```
identifier
  = !reserved id:$(identifier_start identifier_char*) {
      return { type: "identifier", name: id, loc: location() };
    }

number
  = n:$("-"? [0-9]+ ("/" [0-9]+)?) {
      return { type: "number", value: n, loc: location() };
    }

// For operators, capture the operator token with its location
pipe_op
  = "|" !("=") { return { op: "|", loc: location() }; }
```

For binary operators like `Pipe`, we capture both the operator location and the overall expression location:

```
pipeline
  = head:concat_term tail:(_ op:pipe_op _ right:concat_term { return { op, right }; })* {
      return buildBinaryLeftWithLoc(head, tail, location());
    }
```

### Step 3: Create Token Extractor Visitor

Create `abct/src/tokenize.ts` - a visitor that walks the AST and produces tokens:

```
interface AbctToken {
  type: AbctTokenType;
  text: string;
  line: number;
  column: number;
  length: number;
}

enum AbctTokenType {
  COMMENT,
  KEYWORD,
  OPERATOR,
  ABC_LITERAL,
  NUMBER,
  IDENTIFIER,
  FILE_REF,
  SELECTOR,
  VARIABLE,
  VOICE_REF,
}

function extractTokens(ast: Program, source: string): AbctToken[]
  tokens = []

  // Extract comments from source (Peggy skips them)
  extractComments(source, tokens)

  // Walk AST to extract positioned tokens
  for statement in ast.statements:
    if isAssignment(statement):
      // Left side is a variable
      tokens.push({ type: VARIABLE, ...statement.id.loc })
      extractExprTokens(statement.value, tokens)
    else:
      extractExprTokens(statement, tokens)

  // Sort by position
  return tokens.sort((a, b) => a.line - b.line || a.column - b.column)

function extractExprTokens(expr: Expr, tokens):
  match expr.type:
    "pipe":
      extractExprTokens(expr.left, tokens)
      tokens.push({ type: OPERATOR, loc: expr.opLoc })
      extractExprTokens(expr.right, tokens)
    "identifier":
      tokens.push({ type: IDENTIFIER, loc: expr.loc })
    "selector":
      tokens.push({ type: SELECTOR, loc: expr.loc })
    "number":
      tokens.push({ type: NUMBER, loc: expr.loc })
    "abc_literal":
      tokens.push({ type: ABC_LITERAL, loc: expr.loc })
    // ... etc for all node types
```

### Step 4: Create AbctDocument Class

Create `abc-lsp-server/src/AbctDocument.ts`:

```
class AbctDocument:
  diagnostics: Diagnostic[]
  tokens: AbctToken[]
  AST: Program | null

  analyze():
    source = document.getText()
    result = parse(source)

    if result.success:
      AST = result.value
      tokens = extractTokens(AST, source)
    else:
      diagnostics.push(toDiagnostic(result.error))

    return tokens
```

### Step 5: Create Token Type Mapping for ABCT

Add to `abc-lsp-server/src/server_helpers.ts`:

```
function mapAbctTokenToScope(type: AbctTokenType): number {
  switch (type) {
    case AbctTokenType.COMMENT: return standardTokenScopes.comment;
    case AbctTokenType.KEYWORD: return standardTokenScopes.keyword;
    case AbctTokenType.OPERATOR: return standardTokenScopes.operator;
    case AbctTokenType.ABC_LITERAL: return standardTokenScopes.string;
    case AbctTokenType.NUMBER: return standardTokenScopes.number;
    case AbctTokenType.IDENTIFIER: return standardTokenScopes.function;
    case AbctTokenType.VARIABLE: return standardTokenScopes.variable;
    case AbctTokenType.FILE_REF: return standardTokenScopes.string;
    case AbctTokenType.SELECTOR: return standardTokenScopes.decorator;
    case AbctTokenType.VOICE_REF: return standardTokenScopes.type;
  }
}
```

### Step 6: Integrate into AbcLspServer

Update `abc-lsp-server/src/AbcLspServer.ts`:

1. Add `AbctDocument` to `DocumentType` union
2. Add `isAbctFile(uri)` method
3. Update `onDidChangeContent` to create `AbctDocument` for `.abct` files
4. Update `onSemanticTokens` to handle AbctDocument's different token format

```
onSemanticTokens(uri):
  doc = abcDocuments.get(uri)
  builder = new SemanticTokensBuilder()

  if doc instanceof AbctDocument:
    for token in doc.tokens:
      builder.push(token.line - 1, token.column - 1, token.length,
                   mapAbctTokenToScope(token.type), 0)
  else:
    // Existing ABC/ABCx logic
    for token in doc.tokens:
      builder.push(token.line, token.position, token.lexeme.length,
                   mapTTtoStandardScope(token.type), 0)

  return builder.build()
```

### Step 7: Write Tests

Create property-based tests in `abct/tests/tokenize.spec.ts` using the existing generators from `generators.ts`:

```
Property tests:
- Every generated program parses and tokenizes without error
- extractTokens(parse(input)) returns non-empty token list
- All tokens have valid line/column (positive, within source bounds)
- Token positions are monotonically increasing (sorted order)
- Total token length coverage approximates source length (minus whitespace)

Example tests (for edge cases):
- Empty program produces empty token list
- Comment-only produces comment tokens
- Nested ABC literals are correctly bounded
```

---

## Files to Create

| File | Purpose |
|------|---------|
| `abct/src/tokenize.ts` | Token extractor visitor |
| `abc-lsp-server/src/AbctDocument.ts` | ABCT document class |
| `abct/tests/tokenize.spec.ts` | Tokenizer tests (property-based using generators) |

---

## Files to Modify

| File | Changes |
|------|---------|
| `abct/src/ast.ts` | Add Loc interface, add loc to all nodes |
| `abct/src/grammar.peggy` | Add location() calls to all rules |
| `abct/src/parser.ts` | Re-export tokenize functions |
| `abc-lsp-server/src/server_helpers.ts` | Add mapAbctTokenToScope |
| `abc-lsp-server/src/AbcLspServer.ts` | Add .abct file handling |

---

## Verification

1. Run property-based tests: `npm run test`
   - The generators produce thousands of valid ABCT programs
   - Each program is parsed and tokenized
   - Token positions and types are validated

2. Example properties to verify:
   - For all valid programs, tokenization succeeds
   - Token list is sorted by position
   - No token positions overlap (except for nesting)
   - Each token type maps to a valid semantic scope

3. Manual verification in VSCode:
   - Open a generated `.abct` file
   - Verify syntax highlighting applies correctly
