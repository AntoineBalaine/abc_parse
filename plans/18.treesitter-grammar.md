# TreeSitter Grammar for ABC Music Notation

## Table of Contents

1. [Overview](#1-overview)
2. [Goals and Non-Goals](#2-goals-and-non-goals)
3. [Architecture](#3-architecture)
4. [Phase 1: Project Setup and Build Integration](#4-phase-1-project-setup-and-build-integration)
5. [Phase 2: Custom Scanner Deep Dive](#5-phase-2-custom-scanner-deep-dive)
6. [Phase 3: Grammar Rules](#6-phase-3-grammar-rules)
7. [Phase 4: Child-Sibling Comparison Framework](#7-phase-4-child-sibling-comparison-framework)
8. [Phase 5: Test Suite](#8-phase-5-test-suite)
9. [Parallelization Strategy](#9-parallelization-strategy)
10. [Workflow](#10-workflow)

---

## 1. Overview

This plan describes the implementation of a TreeSitter grammar for ABC music notation. The grammar will enable syntax highlighting and structure-aware editing.

The existing recursive descent parser in `parse/parsers/` serves as the reference implementation. We will validate correctness by comparing the TreeSitter output against the existing parser's output using a child-sibling tree comparison framework.

Because ABC notation has context-sensitive tokenization, we will implement a custom external scanner in C that mirrors the architecture of the existing TypeScript scanner, using a context struct and boolean sub-functions.

---

## 2. Goals and Non-Goals

### Goals

- Produce a TreeSitter grammar that parses valid ABC notation
- Maintain node naming consistent with `Expr2.ts`
- Implement a custom external scanner that mirrors the TypeScript scanner's context-based architecture
- Handle ALL 84 token types from the TT enum
- Reuse the existing property-based testing framework (fast-check) and example_scores for validation
- Integrate into the abc_parse npm workspace so `npm run build` triggers the TreeSitter build

### Non-Goals

- ABCx extensions (not part of this implementation)
- Editor integration (not requested)
- Parsing info line content semantically (deferred to semantic layer)
- Beam grouping (deferred to semantic layer)
- Macro expansion (runtime concern)

---

## 3. Architecture

### Directory Structure

```
abc_parse/
├── tree-sitter-abc/
│   ├── grammar.js              # TreeSitter grammar definition
│   ├── src/
│   │   ├── parser.c            # Generated parser (do not edit)
│   │   ├── scanner.c           # Custom scanner (mirrors TypeScript scanner)
│   │   ├── scanner.h           # Scanner header with context struct
│   │   └── tree_sitter/
│   │       └── parser.h        # Generated header
│   ├── bindings/
│   │   └── node/               # Node.js bindings
│   ├── package.json
│   └── binding.gyp
├── parse/
│   ├── comparison/
│   │   ├── CSNode.ts           # Child-sibling node type
│   │   ├── exprToCS.ts         # Convert Expr AST to child-sibling
│   │   ├── tsToCS.ts           # Convert TreeSitter AST to child-sibling
│   │   └── compare.ts          # Comparison algorithm
│   └── tests/
│       └── treesitter/
│           ├── comparison.spec.ts      # Comparison tests
│           └── pbt_treesitter.spec.ts  # Property-based tests
```

### Data Flow

```
ABC Source Text
      │
      ├─────────────────────────────────────┐
      │                                     │
      ▼                                     ▼
[Existing Parser]                    [TreeSitter Parser]
   (TypeScript)                      (C scanner + grammar.js)
      │                                     │
      ▼                                     ▼
  Expr AST                           TreeSitter Tree
      │                                     │
      ▼                                     ▼
[exprToCS]                              [tsToCS]
      │                                     │
      ▼                                     ▼
   CSNode ◄──────── compare() ──────► CSNode
```

---

## 4. Phase 1: Project Setup and Build Integration

### 4.1 NPM Workspace Integration

The tree-sitter-abc package will be a workspace member of abc_parse.

Root `package.json` updates:
```json
{
  "workspaces": ["parse", "tree-sitter-abc"],
  "scripts": {
    "build": "npm run build -w parse && npm run build -w tree-sitter-abc",
    "test": "npm run test -w parse && npm run test -w tree-sitter-abc"
  }
}
```

### 4.2 tree-sitter-abc/package.json

```json
{
  "name": "tree-sitter-abc",
  "version": "1.0.0",
  "main": "bindings/node",
  "scripts": {
    "generate": "tree-sitter generate",
    "build": "tree-sitter generate && node-gyp rebuild",
    "test": "tree-sitter test"
  },
  "devDependencies": {
    "tree-sitter-cli": "^0.22.0",
    "nan": "^2.18.0"
  }
}
```

### 4.3 binding.gyp

```python
{
  "targets": [{
    "target_name": "tree_sitter_abc_binding",
    "include_dirs": ["<!(node -e \"require('nan')\")", "src"],
    "sources": [
      "bindings/node/binding.cc",
      "src/parser.c",
      "src/scanner.c"
    ],
    "cflags_c": ["-std=c99"]
  }]
}
```

### 4.4 Files to Create

- `tree-sitter-abc/grammar.js` - initial placeholder
- `tree-sitter-abc/package.json`
- `tree-sitter-abc/binding.gyp`
- `tree-sitter-abc/bindings/node/binding.cc` - standard TreeSitter node binding
- `tree-sitter-abc/src/scanner.h`
- `tree-sitter-abc/src/scanner.c`

---

## 5. Phase 2: Custom Scanner Deep Dive

### 5.1 TSLexer API

TreeSitter provides the TSLexer struct for external scanners:

```c
struct TSLexer {
  int32_t lookahead;              // Current character (Unicode codepoint)
  TSSymbol result_symbol;         // Set this to the token type when matched

  void (*advance)(TSLexer *, bool skip);   // Consume character
  void (*mark_end)(TSLexer *);             // Mark token boundary
  uint32_t (*get_column)(TSLexer *);       // Column position
  bool (*eof)(const TSLexer *);            // Check end of file
};
```

Key behaviors:
- `lookahead` is ONE character ahead - no built-in multi-char lookahead
- `advance(lexer, false)` consumes character into token
- `advance(lexer, true)` skips character (whitespace)
- `mark_end(lexer)` sets token boundary - subsequent advances don't extend token
- Must set `lexer->result_symbol` before returning true

### 5.2 Problem with Direct TSLexer Usage

Using TSLexer directly in sub-functions is impractical because:
1. No way to "test" a pattern without consuming
2. No way to backtrack after advancing
3. Multi-char lookahead requires manual state management
4. No regex support

### 5.3 PCRE2 Dependency

The TypeScript scanner relies heavily on regex patterns like `/[a-gA-G]/`, `/[0-9]+/`, `/(\^[\^\/]?)|(_[_\/]?)|=/`. To replicate this in C, we use [PCRE2](https://github.com/PCRE2Project/pcre2).

#### Installation

PCRE2 is available via package managers:
- macOS: `brew install pcre2`
- Ubuntu/Debian: `apt-get install libpcre2-dev`
- Windows: vcpkg or manual build

#### Build Integration

Update `binding.gyp` to link PCRE2:

```python
{
  "targets": [{
    "target_name": "tree_sitter_abc_binding",
    "include_dirs": [
      "<!(node -e \"require('nan')\")",
      "src",
      "<!(pkg-config --cflags-only-I pcre2-8 | sed 's/-I//g')"
    ],
    "sources": [
      "bindings/node/binding.cc",
      "src/parser.c",
      "src/scanner.c"
    ],
    "cflags_c": ["-std=c99"],
    "libraries": ["<!(pkg-config --libs pcre2-8)"]
  }]
}
```

#### PCRE2 API Usage

```c
#define PCRE2_CODE_UNIT_WIDTH 8
#include <pcre2.h>

// Compile a pattern once, reuse for matching
typedef struct {
  pcre2_code *pattern;
  pcre2_match_data *match_data;
} CompiledPattern;

// Initialize pattern (PCRE2_ANCHORED passed at match time, not compile time)
CompiledPattern *compile_pattern(const char *regex) {
  int errcode;
  PCRE2_SIZE erroffset;
  pcre2_code *code = pcre2_compile(
    (PCRE2_SPTR)regex,
    PCRE2_ZERO_TERMINATED,
    0,  // No compile-time flags
    &errcode, &erroffset, NULL
  );
  if (!code) return NULL;

  CompiledPattern *cp = malloc(sizeof(CompiledPattern));
  cp->pattern = code;
  cp->match_data = pcre2_match_data_create_from_pattern(code, NULL);
  return cp;
}

// Test if pattern matches at current position (like ctx.test())
bool pattern_test(CompiledPattern *cp, const char *subject, size_t offset, size_t length) {
  int rc = pcre2_match(
    cp->pattern,
    (PCRE2_SPTR)subject,
    length,
    offset,
    PCRE2_ANCHORED,  // Match must start at offset
    cp->match_data,
    NULL
  );
  return rc >= 0;
}

// Match and return length of match (0 if no match)
size_t pattern_match(CompiledPattern *cp, const char *subject, size_t offset, size_t length) {
  int rc = pcre2_match(
    cp->pattern,
    (PCRE2_SPTR)subject,
    length,
    offset,
    PCRE2_ANCHORED,  // Match must start at offset
    cp->match_data,
    NULL
  );
  if (rc < 0) return 0;

  PCRE2_SIZE *ovector = pcre2_get_ovector_pointer(cp->match_data);
  return ovector[1] - ovector[0];  // End - Start = length of match
}
```

### 5.4 Solution: Mirror the TypeScript Scanner Architecture

We create a `ScanCtx` struct that mirrors the TypeScript `Ctx`, including pre-compiled PCRE2 patterns:

```c
// scanner.h
#ifndef TREE_SITTER_ABC_SCANNER_H
#define TREE_SITTER_ABC_SCANNER_H

#include <tree_sitter/parser.h>
#include <stdbool.h>
#include <stdint.h>
#include <string.h>
#include <stdlib.h>

#define PCRE2_CODE_UNIT_WIDTH 8
#include <pcre2.h>

// Token types - must match TT enum from TypeScript (84 tokens)
typedef enum {
  // Pitch and Music Elements
  TT_ACCIDENTAL,
  TT_NOTE_LETTER,
  TT_OCTAVE,
  TT_REST,
  TT_TIE,
  TT_DECORATION,
  TT_SLUR,
  TT_BARLINE,

  // Rhythmic Elements
  TT_RHY_NUMER,
  TT_RHY_DENOM,
  TT_RHY_SEP,
  TT_RHY_BRKN,
  TT_TUPLET_LPAREN,
  TT_TUPLET_P,
  TT_TUPLET_COLON,
  TT_TUPLET_Q,
  TT_TUPLET_R,
  TT_REPEAT_NUMBER,
  TT_REPEAT_COMMA,
  TT_REPEAT_DASH,
  TT_REPEAT_X,

  // Structural Brackets
  TT_CHRD_LEFT_BRKT,
  TT_CHRD_RIGHT_BRKT,
  TT_GRC_GRP_LEFT_BRACE,
  TT_GRC_GRP_RGHT_BRACE,
  TT_GRC_GRP_SLSH,
  TT_INLN_FLD_LFT_BRKT,
  TT_INLN_FLD_RGT_BRKT,

  // Generic Punctuation (for directive/info line contexts)
  TT_EQL,        // =
  TT_SLASH,      // /
  TT_MINUS,      // -
  TT_PLUS,       // +
  TT_LPAREN,     // (
  TT_RPAREN,     // )
  TT_LBRACE,     // {
  TT_RBRACE,     // }
  TT_LBRACKET,   // [
  TT_RBRACKET,   // ]
  TT_PIPE,       // | (directive context, not barline)

  // Information Fields
  TT_ANNOTATION,
  TT_INF_HDR,
  TT_INFO_STR,
  TT_INF_CTND,
  TT_VOICE,
  TT_VOICE_OVRLAY,
  TT_LINE_CONT,

  // Symbols and Special
  TT_SYMBOL,
  TT_USER_SY,
  TT_USER_SY_HDR,
  TT_USER_SY_INVOCATION,
  TT_MACRO_HDR,
  TT_MACRO_STR,
  TT_MACRO_INVOCATION,
  TT_MACRO_VAR,

  // Lyrics
  TT_LY_HDR,
  TT_LY_TXT,
  TT_LY_UNDR,
  TT_LY_HYPH,
  TT_LY_SECT_HDR,
  TT_LY_SPS,
  TT_LY_STAR,

  // Symbol Line
  TT_SY_HDR,
  TT_SY_STAR,
  TT_SY_TXT,

  // Directives
  TT_STYLESHEET_DIRECTIVE,
  TT_MEASUREMENT_UNIT,

  // Utility
  TT_AMPERSAND,
  TT_SYSTEM_BREAK,
  TT_BCKTCK_SPC,
  TT_Y_SPC,
  TT_SPECIAL_LITERAL,

  // General
  TT_IDENTIFIER,
  TT_NUMBER,
  TT_RESERVED_CHAR,
  TT_ESCAPED_CHAR,
  TT_CHORD_SYMBOL,   // ABCx chord symbol
  TT_DISCARD,        // used only by generators

  // Structural
  TT_COMMENT,
  TT_WS,
  TT_EOL,
  TT_FREE_TXT,
  TT_SCT_BRK,
  TT_INVALID,
  TT_EOF,

  TT_COUNT  // Total count: 84
} TokenType;

// Pre-compiled regex patterns (initialized once in create())
typedef struct {
  pcre2_code *code;
  pcre2_match_data *match_data;
} Pattern;

// Pattern IDs for the registry
typedef enum {
  PAT_NOTE_LETTER,     // [a-gA-G]
  PAT_OCTAVE,          // [',]+
  PAT_ACCIDENTAL,      // (\^[\^\/]?)|(_[_\/]?)|=
  PAT_REST,            // [zZxX]
  PAT_DIGIT,           // [0-9]
  PAT_DIGITS,          // [0-9]+
  PAT_NUMBER,          // [0-9]+(\.[0-9]+)?
  PAT_BROKEN_RHYTHM,   // [><]+
  PAT_DECORATION,      // [.~HLMOPRSTuv]+
  PAT_WS,              // [ \t]+
  PAT_EOL,             // \r?\n
  PAT_IDENTIFIER,      // [a-zA-Z_][a-zA-Z0-9_]*
  PAT_INFO_HDR,        // [A-Za-z][ \t]*:
  PAT_ANNOTATION,      // "([^"\\]|\\.)*"
  PAT_SYMBOL,          // ![^\n!]+!|\+[^\n+]+\+
  PAT_COUNT
} PatternId;

// Simple string map for macros/user-symbols (linked list for simplicity)
typedef struct StringMapEntry {
  char *key;
  char *value;
  struct StringMapEntry *next;
} StringMapEntry;

typedef struct {
  StringMapEntry *head;
} StringMap;

// Scanner context - mirrors TypeScript Ctx
typedef struct {
  const char *source;       // Input text (UTF-8)
  size_t source_len;        // Length of source
  size_t source_capacity;   // Allocated capacity
  size_t start;             // Start of current token
  size_t current;           // Current cursor position
  int line;                 // 0-based line number
  size_t line_start;        // Byte offset of line start

  // State for multi-line constructs
  bool in_text_block;       // Inside %%begintext...%%endtext
  bool in_tune_body;        // Inside tune body vs header

  // Context-sensitive maps (like TypeScript Ctx)
  StringMap *macros;        // Macro declarations
  StringMap *user_symbols;  // User-defined symbols

  // Pre-compiled regex patterns
  Pattern patterns[PAT_COUNT];

  // Reference to TSLexer for final output
  TSLexer *lexer;
} ScanCtx;

// Context operations
void ctx_init(ScanCtx *ctx, TSLexer *lexer);
void ctx_init_patterns(ScanCtx *ctx);  // Compile all regex patterns
void ctx_free_patterns(ScanCtx *ctx);  // Free compiled patterns
bool ctx_test_char(ScanCtx *ctx, char c);
bool ctx_test_str(ScanCtx *ctx, const char *str);
bool ctx_test_regex(ScanCtx *ctx, PatternId pat);  // Test regex at current position
size_t ctx_match_regex(ScanCtx *ctx, PatternId pat);  // Match and return length
void ctx_advance(ScanCtx *ctx, size_t count);
char ctx_peek(ScanCtx *ctx);
char ctx_peek_next(ScanCtx *ctx);
bool ctx_is_at_end(ScanCtx *ctx);
void ctx_emit(ScanCtx *ctx, TokenType type);

// Sub-scanner functions (boolean, like TypeScript)
bool scan_comment(ScanCtx *ctx);
bool scan_ws(ScanCtx *ctx);
bool scan_eol(ScanCtx *ctx);
bool scan_annotation(ScanCtx *ctx);
bool scan_inline_field(ScanCtx *ctx);
bool scan_chord(ScanCtx *ctx);
bool scan_grace_group(ScanCtx *ctx);
bool scan_tuplet(ScanCtx *ctx);
bool scan_barline(ScanCtx *ctx);
bool scan_decoration(ScanCtx *ctx);
bool scan_note(ScanCtx *ctx);
bool scan_pitch(ScanCtx *ctx);
bool scan_rhythm(ScanCtx *ctx);
bool scan_rest(ScanCtx *ctx);
bool scan_tie(ScanCtx *ctx);
bool scan_slur(ScanCtx *ctx);
bool scan_symbol(ScanCtx *ctx);
bool scan_ampersand(ScanCtx *ctx);
bool scan_line_continuation(ScanCtx *ctx);
bool scan_info_line(ScanCtx *ctx);
bool scan_lyric_line(ScanCtx *ctx);
bool scan_symbol_line(ScanCtx *ctx);
bool scan_directive(ScanCtx *ctx);
bool scan_system_break(ScanCtx *ctx);
bool scan_y_spacer(ScanCtx *ctx);
bool scan_bcktck_spacer(ScanCtx *ctx);
bool scan_repeat_numbers(ScanCtx *ctx);

// Error recovery
bool collect_invalid_token(ScanCtx *ctx);
bool is_recovery_point(ScanCtx *ctx);

#endif
```

### 5.5 Context Operations Implementation

```c
// scanner.c - context operations

// Pattern definitions (regex strings)
static const char *PATTERN_STRINGS[PAT_COUNT] = {
  [PAT_NOTE_LETTER]   = "[a-gA-G]",
  [PAT_OCTAVE]        = "[',]+",
  [PAT_ACCIDENTAL]    = "(\\^[\\^\\/]?)|(_[_\\/]?)|=",
  [PAT_REST]          = "[zZxX]",
  [PAT_DIGIT]         = "[0-9]",
  [PAT_DIGITS]        = "[0-9]+",
  [PAT_NUMBER]        = "[0-9]+(\\.[0-9]+)?",
  [PAT_BROKEN_RHYTHM] = "[><]+",
  [PAT_DECORATION]    = "[.~HLMOPRSTuv]+",
  [PAT_WS]            = "[ \\t]+",
  [PAT_EOL]           = "\\r?\\n",
  [PAT_IDENTIFIER]    = "[a-zA-Z_][a-zA-Z0-9_]*",
  [PAT_INFO_HDR]      = "[A-Za-z][ \\t]*:",
  [PAT_ANNOTATION]    = "\"([^\"\\\\]|\\\\.)*\"",
  [PAT_SYMBOL]        = "![^\\n!]+!|\\+[^\\n+]+\\+",
};

// Initialize all patterns (called once in create())
void ctx_init_patterns(ScanCtx *ctx) {
  int errcode;
  PCRE2_SIZE erroffset;

  for (int i = 0; i < PAT_COUNT; i++) {
    ctx->patterns[i].code = pcre2_compile(
      (PCRE2_SPTR)PATTERN_STRINGS[i],
      PCRE2_ZERO_TERMINATED,
      0,  // No compile-time flags; PCRE2_ANCHORED passed at match time
      &errcode, &erroffset, NULL
    );
    if (ctx->patterns[i].code) {
      ctx->patterns[i].match_data =
        pcre2_match_data_create_from_pattern(ctx->patterns[i].code, NULL);
    }
  }
}

// Free all patterns (called in destroy())
void ctx_free_patterns(ScanCtx *ctx) {
  for (int i = 0; i < PAT_COUNT; i++) {
    if (ctx->patterns[i].match_data) {
      pcre2_match_data_free(ctx->patterns[i].match_data);
    }
    if (ctx->patterns[i].code) {
      pcre2_code_free(ctx->patterns[i].code);
    }
  }
}

void ctx_init(ScanCtx *ctx, TSLexer *lexer) {
  ctx->lexer = lexer;
  ctx->start = 0;
  ctx->current = 0;
  ctx->line = 0;
  ctx->line_start = 0;
  ctx->in_text_block = false;
  ctx->in_tune_body = false;
  // Note: source is filled by reading from lexer
  // Note: patterns are initialized in create(), not here
}

// Test if regex matches at current position (like ctx.test(regex) in TypeScript)
bool ctx_test_regex(ScanCtx *ctx, PatternId pat) {
  if (ctx_is_at_end(ctx)) return false;
  Pattern *p = &ctx->patterns[pat];
  if (!p->code) return false;

  int rc = pcre2_match(
    p->code,
    (PCRE2_SPTR)ctx->source,
    ctx->source_len,
    ctx->current,  // Start at current position
    PCRE2_ANCHORED,  // Match must start at ctx->current
    p->match_data,
    NULL
  );
  return rc >= 0;
}

// Match regex and return length of match (0 if no match)
// This is like matchPattern() in TypeScript - it advances if matched
size_t ctx_match_regex(ScanCtx *ctx, PatternId pat) {
  if (ctx_is_at_end(ctx)) return 0;
  Pattern *p = &ctx->patterns[pat];
  if (!p->code) return 0;

  int rc = pcre2_match(
    p->code,
    (PCRE2_SPTR)ctx->source,
    ctx->source_len,
    ctx->current,
    PCRE2_ANCHORED,  // Match must start at ctx->current
    p->match_data,
    NULL
  );
  if (rc < 0) return 0;

  PCRE2_SIZE *ovector = pcre2_get_ovector_pointer(p->match_data);
  return ovector[1] - ovector[0];  // Length of match
}

// Test if current position matches a single character
bool ctx_test_char(ScanCtx *ctx, char c) {
  if (ctx_is_at_end(ctx)) return false;
  return ctx->source[ctx->current] == c;
}

// Test if current position matches a string literal
bool ctx_test_str(ScanCtx *ctx, const char *str) {
  size_t len = strlen(str);
  if (ctx->current + len > ctx->source_len) return false;
  return strncmp(&ctx->source[ctx->current], str, len) == 0;
}

// Test if current char is in a character set
bool ctx_test_charset(ScanCtx *ctx, const char *charset) {
  if (ctx_is_at_end(ctx)) return false;
  return strchr(charset, ctx->source[ctx->current]) != NULL;
}

// Advance cursor by count bytes
void ctx_advance(ScanCtx *ctx, size_t count) {
  ctx->current += count;
}

// Peek at current character
char ctx_peek(ScanCtx *ctx) {
  if (ctx_is_at_end(ctx)) return '\0';
  return ctx->source[ctx->current];
}

// Peek at next character
char ctx_peek_next(ScanCtx *ctx) {
  if (ctx->current + 1 >= ctx->source_len) return '\0';
  return ctx->source[ctx->current + 1];
}

// Check if at end of input
bool ctx_is_at_end(ScanCtx *ctx) {
  return ctx->current >= ctx->source_len;
}

// Emit a token - syncs with TSLexer
void ctx_emit(ScanCtx *ctx, TokenType type) {
  // Advance TSLexer to match our context
  size_t chars_to_advance = ctx->current - ctx->start;
  for (size_t i = 0; i < chars_to_advance; i++) {
    ctx->lexer->advance(ctx->lexer, false);
  }
  ctx->lexer->mark_end(ctx->lexer);
  ctx->lexer->result_symbol = type;
  ctx->start = ctx->current;
}
```

### 5.5 Sub-Scanner Pattern (Mirroring TypeScript)

Each sub-scanner follows the boolean pattern:

```c
// Example: scan_ampersand mirrors TypeScript ampersand()
bool scan_ampersand(ScanCtx *ctx) {
  if (!ctx_test_char(ctx, '&')) return false;

  // Check if followed by newline
  if (ctx_test_str(ctx, "&\n")) {
    ctx_advance(ctx, 2);
    ctx_emit(ctx, TT_VOICE_OVRLAY);
    ctx->line++;
    ctx->line_start = ctx->current;
    return true;
  }

  ctx_advance(ctx, 1);
  ctx_emit(ctx, TT_VOICE);
  return true;
}

// Example: scan_decoration with lookahead
bool scan_decoration(ScanCtx *ctx) {
  if (!ctx_test_charset(ctx, ".~HLMOPRSTuv")) return false;

  size_t start_pos = ctx->current;

  // Consume decoration characters
  while (ctx_test_charset(ctx, ".~HLMOPRSTuv")) {
    ctx_advance(ctx, 1);
  }

  // Lookahead: must be followed by pitch, rest, or chord
  if (!is_pitch_start(ctx) && !is_rest_start(ctx) && !ctx_test_char(ctx, '[')) {
    // Backtrack - not a decoration
    ctx->current = start_pos;
    return false;
  }

  ctx_emit(ctx, TT_DECORATION);
  return true;
}

// Helper: check if at pitch start
bool is_pitch_start(ScanCtx *ctx) {
  char c = ctx_peek(ctx);
  return (c >= 'a' && c <= 'g') || (c >= 'A' && c <= 'G') ||
         c == '^' || c == '_' || c == '=';
}
```

### 5.6 Main Scanner Loop

```c
// The main scan function - called by TreeSitter
bool tree_sitter_abc_external_scanner_scan(
  void *payload,
  TSLexer *lexer,
  const bool *valid_symbols
) {
  ScanCtx *ctx = (ScanCtx *)payload;
  ctx->lexer = lexer;

  // Read source from lexer into context
  // (implementation depends on how we buffer input)

  ctx->start = ctx->current;

  // Try each scanner in order (mirrors TypeScript while loop)
  if (scan_comment(ctx)) return true;
  if (scan_annotation(ctx)) return true;
  if (scan_inline_field(ctx)) return true;
  if (scan_chord(ctx)) return true;
  if (scan_grace_group(ctx)) return true;
  if (scan_tuplet(ctx)) return true;
  if (scan_barline(ctx)) return true;
  if (scan_decoration(ctx)) return true;
  if (scan_note(ctx)) return true;
  if (scan_rest(ctx)) return true;
  if (scan_tie(ctx)) return true;
  if (scan_symbol(ctx)) return true;
  if (scan_ampersand(ctx)) return true;
  if (scan_line_continuation(ctx)) return true;
  if (scan_ws(ctx)) return true;
  if (scan_eol(ctx)) return true;

  // Error recovery
  if (collect_invalid_token(ctx)) return true;

  return false;
}
```

### 5.7 Handling the TSLexer Limitation

The key insight is that we buffer the input ourselves:

1. In `create()`: allocate a ScanCtx with a source buffer
2. In `scan()`: read characters from TSLexer into our buffer as needed
3. Use our context for all pattern matching (test/advance/backtrack)
4. Only sync with TSLexer when emitting a token via `ctx_emit()`

```c
// Read from TSLexer into our buffer
void fill_buffer(ScanCtx *ctx) {
  // Advance TSLexer and copy characters to our source buffer
  // This allows us to do arbitrary lookahead and backtracking
  while (!ctx->lexer->eof(ctx->lexer)) {
    char c = (char)ctx->lexer->lookahead;
    // Append to ctx->source buffer
    ctx->lexer->advance(ctx->lexer, true);  // skip mode
  }
}
```

### 5.8 State Serialization

The serialization must handle:
- Boolean flags (in_text_block, in_tune_body)
- Line number
- Macro and user-symbol declarations (variable-length)

Because TreeSitter's serialize buffer has a limit (TREE_SITTER_SERIALIZATION_BUFFER_SIZE, typically 1024 bytes), we adopt a simplified strategy:

1. Serialize boolean flags and line number (fixed 4 bytes)
2. Serialize macro count, followed by each macro's key/value lengths and data
3. Serialize user-symbol count, followed by each symbol's key/value lengths and data
4. If data exceeds buffer size, truncate (rare edge case for files with many macros)

```c
unsigned tree_sitter_abc_external_scanner_serialize(void *payload, char *buffer) {
  ScanCtx *ctx = (ScanCtx *)payload;
  unsigned pos = 0;

  // Fixed state (4 bytes)
  buffer[pos++] = ctx->in_text_block;
  buffer[pos++] = ctx->in_tune_body;
  buffer[pos++] = (ctx->line >> 8) & 0xFF;
  buffer[pos++] = ctx->line & 0xFF;

  // Macro count
  uint8_t macro_count = 0;
  for (StringMapEntry *e = ctx->macros->head; e && macro_count < 255; e = e->next) {
    macro_count++;
  }
  buffer[pos++] = macro_count;

  // Macro entries (key_len, val_len, key, value)
  for (StringMapEntry *e = ctx->macros->head; e && pos < 1000; e = e->next) {
    size_t key_len = strlen(e->key);
    size_t val_len = strlen(e->value);
    if (pos + 2 + key_len + val_len > 1000) break;
    buffer[pos++] = (uint8_t)key_len;
    buffer[pos++] = (uint8_t)val_len;
    memcpy(buffer + pos, e->key, key_len);
    pos += key_len;
    memcpy(buffer + pos, e->value, val_len);
    pos += val_len;
  }

  // Similar for user_symbols (omitted for brevity)

  return pos;
}

void tree_sitter_abc_external_scanner_deserialize(void *payload, const char *buffer, unsigned length) {
  ScanCtx *ctx = (ScanCtx *)payload;
  if (length < 5) return;

  unsigned pos = 0;
  ctx->in_text_block = buffer[pos++];
  ctx->in_tune_body = buffer[pos++];
  ctx->line = (buffer[pos] << 8) | buffer[pos + 1];
  pos += 2;

  // Clear and rebuild macros
  stringmap_clear(ctx->macros);
  uint8_t macro_count = buffer[pos++];
  for (uint8_t i = 0; i < macro_count && pos < length; i++) {
    uint8_t key_len = buffer[pos++];
    uint8_t val_len = buffer[pos++];
    if (pos + key_len + val_len > length) break;
    char *key = strndup(buffer + pos, key_len);
    pos += key_len;
    char *val = strndup(buffer + pos, val_len);
    pos += val_len;
    stringmap_set(ctx->macros, key, val);
    free(key);
    free(val);
  }

  // Similar for user_symbols
}
```

### 5.9 Buffer Strategy and Incremental Parsing

TreeSitter supports incremental parsing, where only the changed portion of a document is re-parsed. The buffer strategy affects this:

Option A: Buffer entire input (simple but defeats incremental parsing)
- Reads all characters from TSLexer into ctx->source at scan start
- Allows arbitrary lookahead and backtracking
- Loses incremental parsing benefits for large files

Option B: On-demand reading with limited lookahead (complex but incremental-friendly)
- Reads characters only as needed
- Maintains a sliding window buffer for lookahead
- Better for large files but more complex implementation

For Phase 1, we use Option A because:
1. ABC files are typically small (< 100KB)
2. Simpler implementation, easier to debug
3. Can be optimized later if performance is an issue

The fill_buffer function reads the entire input once:

```c
void fill_buffer(ScanCtx *ctx) {
  // Reset TSLexer position tracking
  size_t capacity = 4096;
  ctx->source = malloc(capacity);
  ctx->source_len = 0;

  while (!ctx->lexer->eof(ctx->lexer)) {
    if (ctx->source_len >= capacity - 1) {
      capacity *= 2;
      ctx->source = realloc((char*)ctx->source, capacity);
    }
    ((char*)ctx->source)[ctx->source_len++] = (char)ctx->lexer->lookahead;
    ctx->lexer->advance(ctx->lexer, true);  // Skip mode to fill buffer
  }
  ((char*)ctx->source)[ctx->source_len] = '\0';
  ctx->source_capacity = capacity;

  // Reset lexer position for actual tokenization
  // Note: TreeSitter will call scan() fresh for each token
}
```

Note: After filling the buffer, we must track TSLexer position separately from our ctx->current position, and synchronize them in ctx_emit().

---

## 6. Phase 3: Grammar Rules

The grammar.js file defines the tree structure. External tokens are handled by the scanner.

### 6.1 External Tokens

All 84 tokens from the TT enum must be declared as externals:

```javascript
module.exports = grammar({
  name: 'abc',

  externals: $ => [
    // Pitch and Music Elements
    $.ACCIDENTAL, $.NOTE_LETTER, $.OCTAVE, $.REST,
    $.TIE, $.DECORATION, $.SLUR, $.BARLINE,

    // Rhythmic Elements
    $.RHY_NUMER, $.RHY_DENOM, $.RHY_SEP, $.RHY_BRKN,
    $.TUPLET_LPAREN, $.TUPLET_P, $.TUPLET_COLON, $.TUPLET_Q, $.TUPLET_R,
    $.REPEAT_NUMBER, $.REPEAT_COMMA, $.REPEAT_DASH, $.REPEAT_X,

    // Structural Brackets
    $.CHRD_LEFT_BRKT, $.CHRD_RIGHT_BRKT,
    $.GRC_GRP_LEFT_BRACE, $.GRC_GRP_RGHT_BRACE, $.GRC_GRP_SLSH,
    $.INLN_FLD_LFT_BRKT, $.INLN_FLD_RGT_BRKT,

    // Generic Punctuation (directive/info line contexts)
    $.EQL, $.SLASH, $.MINUS, $.PLUS,
    $.LPAREN, $.RPAREN, $.LBRACE, $.RBRACE,
    $.LBRACKET, $.RBRACKET, $.PIPE,

    // Information Fields
    $.ANNOTATION, $.INF_HDR, $.INFO_STR, $.INF_CTND,
    $.VOICE, $.VOICE_OVRLAY, $.LINE_CONT,

    // Symbols and Special
    $.SYMBOL, $.USER_SY, $.USER_SY_HDR, $.USER_SY_INVOCATION,
    $.MACRO_HDR, $.MACRO_STR, $.MACRO_INVOCATION, $.MACRO_VAR,

    // Lyrics
    $.LY_HDR, $.LY_TXT, $.LY_UNDR, $.LY_HYPH,
    $.LY_SECT_HDR, $.LY_SPS, $.LY_STAR,

    // Symbol Line
    $.SY_HDR, $.SY_STAR, $.SY_TXT,

    // Directives
    $.STYLESHEET_DIRECTIVE, $.MEASUREMENT_UNIT,

    // Utility
    $.AMPERSAND, $.SYSTEM_BREAK, $.BCKTCK_SPC, $.Y_SPC,
    $.SPECIAL_LITERAL,

    // General
    $.IDENTIFIER, $.NUMBER, $.RESERVED_CHAR, $.ESCAPED_CHAR,
    $.CHORD_SYMBOL, $.DISCARD,

    // Structural
    $.COMMENT, $.WS, $.EOL, $.FREE_TXT,
    $.SCT_BRK, $.INVALID, $.EOF,
  ],

  extras: $ => [],  // We handle whitespace explicitly

  rules: {
    // Top level
    File_structure: $ => seq(
      optional($.File_header),
      repeat(choice($.Tune, $.SCT_BRK))
    ),

    // ... rest of grammar rules matching Expr2.ts structure
  }
});
```

### 6.2 Node Types Mapping

All nodes match Expr2.ts names exactly - see Section 3 of the original plan for the complete mapping table.

---

## 7. Phase 4: Child-Sibling Comparison Framework

### 7.1 CSNode Type

```typescript
export interface CSNode {
  type: string;
  text?: string;
  firstChild: CSNode | null;
  nextSibling: CSNode | null;
  // Position info for debugging (optional but helpful)
  startOffset?: number;
  endOffset?: number;
}
```

Note: Position information helps identify where differences occur in the source when debugging comparison failures.

### 7.2 Conversion Functions

`exprToCS.ts` and `tsToCS.ts` convert both AST formats to CSNode for comparison.

### 7.3 Comparison Algorithm

The comparison uses two-way recursion comparing firstChild and nextSibling chains:

```typescript
export interface CompareResult {
  equal: boolean;
  path?: string[];       // Path to first difference
  expected?: string;     // Expected type/text
  actual?: string;       // Actual type/text
}

export function compareCSNodes(a: CSNode | null, b: CSNode | null, path: string[] = []): CompareResult {
  // Both null = equal
  if (a === null && b === null) {
    return { equal: true };
  }

  // One null, other not = unequal
  if (a === null || b === null) {
    return {
      equal: false,
      path,
      expected: a ? a.type : 'null',
      actual: b ? b.type : 'null'
    };
  }

  // Compare types
  if (a.type !== b.type) {
    return {
      equal: false,
      path,
      expected: a.type,
      actual: b.type
    };
  }

  // Compare text for leaf nodes (tokens)
  if (a.text !== undefined || b.text !== undefined) {
    if (a.text !== b.text) {
      return {
        equal: false,
        path,
        expected: a.text ?? '(no text)',
        actual: b.text ?? '(no text)'
      };
    }
  }

  // Recursively compare firstChild
  const childResult = compareCSNodes(a.firstChild, b.firstChild, [...path, a.type, 'firstChild']);
  if (!childResult.equal) return childResult;

  // Recursively compare nextSibling
  const siblingResult = compareCSNodes(a.nextSibling, b.nextSibling, [...path, a.type, 'nextSibling']);
  return siblingResult;
}
```

### 7.4 Handling TreeSitter Anonymous Nodes

TreeSitter generates both named and anonymous nodes. Anonymous nodes (like punctuation `[`, `]`, `|`) might not have direct equivalents in the Expr AST. The `tsToCS` converter must:

1. Skip anonymous nodes that are purely structural (brackets, punctuation)
2. Or map them to equivalent Expr types if they carry semantic meaning

```typescript
export function tsToCS(node: SyntaxNode): CSNode | null {
  // Skip anonymous nodes (type starts with lowercase or is punctuation)
  if (node.isNamed === false) {
    // Continue to next sibling
    return node.nextSibling ? tsToCS(node.nextSibling) : null;
  }

  return {
    type: node.type,
    text: node.childCount === 0 ? node.text : undefined,
    firstChild: node.firstChild ? tsToCS(node.firstChild) : null,
    nextSibling: node.nextSibling ? tsToCS(node.nextSibling) : null,
    startOffset: node.startIndex,
    endOffset: node.endIndex
  };
}
```

---

## 8. Phase 5: Test Suite

### 8.1 Property-Based Testing with fast-check

We need to create ABC-specific generators (the existing generators in `abct/tests/generators.ts` are for the ABCT transformation language, not ABC music notation).

Create `parse/tests/treesitter/generators.ts`:

```typescript
import fc from 'fast-check';

// Generate valid note letters
export const genNoteLetter = fc.constantFrom('a', 'b', 'c', 'd', 'e', 'f', 'g', 'A', 'B', 'C', 'D', 'E', 'F', 'G');

// Generate accidentals
export const genAccidental = fc.constantFrom('', '^', '_', '=', '^^', '__');

// Generate octave modifiers
export const genOctave = fc.stringOf(fc.constantFrom("'", ","), { minLength: 0, maxLength: 3 });

// Generate a simple note
export const genNote = fc.tuple(genAccidental, genNoteLetter, genOctave).map(([acc, note, oct]) => `${acc}${note}${oct}`);

// Generate rhythm value
export const genRhythm = fc.oneof(
  fc.constant(''),
  fc.integer({ min: 1, max: 16 }).map(n => n.toString()),
  fc.integer({ min: 1, max: 8 }).map(n => `/${n}`),
  fc.tuple(fc.integer({ min: 1, max: 8 }), fc.integer({ min: 1, max: 8 })).map(([n, d]) => `${n}/${d}`)
);

// Generate a note with rhythm
export const genNoteWithRhythm = fc.tuple(genNote, genRhythm).map(([n, r]) => `${n}${r}`);

// Generate a barline
export const genBarline = fc.constantFrom('|', '||', '|]', '[|', ':|', '|:', '::', '|1', '|2');

// Generate music content (sequence of notes and barlines)
export const genMusicContent = fc.array(
  fc.oneof(genNoteWithRhythm, genBarline),
  { minLength: 1, maxLength: 20 }
).map(items => items.join(' '));

// Generate info line
export const genInfoLine = fc.tuple(
  fc.constantFrom('T', 'M', 'L', 'K', 'C', 'Q'),
  fc.string({ minLength: 1, maxLength: 20 })
).map(([key, val]) => `${key}:${val}`);

// Generate a minimal tune
export const genTune = fc.tuple(
  fc.integer({ min: 1, max: 100 }),
  fc.string({ minLength: 1, maxLength: 30 }),
  genMusicContent
).map(([num, title, music]) => `X:${num}\nT:${title}\nK:C\n${music}\n`);

// Generate valid ABC syntax
export const genValidABCSyntax = fc.oneof(genTune, genMusicContent, genInfoLine);
```

Create `parse/tests/treesitter/pbt_treesitter.spec.ts`:

```typescript
import fc from 'fast-check';
import { genValidABCSyntax } from './generators';  // Use new ABC-specific generators
import { parseWithBoth, compareCSNodes } from './helpers';

describe('TreeSitter PBT', () => {
  it('parses all valid ABC syntax identically to TypeScript parser', () => {
    fc.assert(
      fc.property(genValidABCSyntax, (input) => {
        const { exprAst, tsTree } = parseWithBoth(input);
        const csFromExpr = exprToCS(exprAst);
        const csFromTS = tsToCS(tsTree.rootNode);
        return compareCSNodes(csFromExpr, csFromTS).equal;
      }),
      { numRuns: 10000 }
    );
  });
});
```

### 8.2 Use example_scores Directory

```typescript
import { glob } from 'glob';
import { readFileSync } from 'fs';

describe('Example scores validation', () => {
  const files = glob.sync('/workspace/abc_parse/example_scores/**/*.abc');

  for (const file of files) {
    it(`parses ${path.basename(file)} identically`, () => {
      const input = readFileSync(file, 'utf-8');
      const { exprAst, tsTree } = parseWithBoth(input);

      const csFromExpr = exprToCS(exprAst);
      const csFromTS = tsToCS(tsTree.rootNode);

      const result = compareCSNodes(csFromExpr, csFromTS);
      expect(result.equal).toBe(true);
    });
  }
});
```

### 8.3 No TreeSitter Corpus Tests

We skip TreeSitter's native test corpus format and rely entirely on the existing test infrastructure.

---

## 9. Parallelization Strategy

### 9.1 Dependency Graph

Because the grammar's externals must match the scanner's token types exactly, we split Phase 2 into two sub-phases:

```
Phase 1 (Setup)
     │
     ▼
Phase 2A (Token Enum)
     │
     ├─────────────────┬─────────────────┐
     ▼                 ▼                 ▼
Phase 2B          Phase 3           Phase 4
(Scanner impl)    (Grammar)         (Comparison)
     │                 │                 │
     └─────────────────┼─────────────────┘
                       ▼
                   Phase 5
                   (Tests)
```

### 9.2 Phase Breakdown

Phase 2A (Token Enum Definition) - MUST complete before parallel work:
- Define the complete TokenType enum in scanner.h
- Define the PatternId enum for regex patterns
- Define the ScanCtx struct

After Phase 2A completes, the following can run in parallel:

1. Agent A: Scanner Implementation (Phase 2B)
   - scanner.c with context operations
   - All sub-scanner functions
   - PCRE2 integration

2. Agent B: Grammar Rules (Phase 3)
   - grammar.js with all node types
   - Externals declaration (uses TokenType enum from Phase 2A)

3. Agent C: Comparison Framework (Phase 4)
   - CSNode.ts
   - exprToCS.ts
   - tsToCS.ts
   - compare.ts

### 9.3 Sequential Dependencies

1. Phase 1 (Setup) must complete first as it creates the project structure
2. Phase 2A must complete before any parallel work can begin
3. Phase 5 (Tests) must wait for Phases 2B, 3, and 4 to complete
4. Each test type has its own dependencies:
   - Scanner tests require Phase 2B
   - Grammar tests require Phases 2B and 3
   - Comparison tests require all phases

### 9.4 Code Review

Each phase gets a code review agent before committing:
- Scanner code review focuses on token type completeness and C correctness
- Grammar code review focuses on node naming consistency and externals completeness
- Comparison code review focuses on CSNode structure matching

---

## 10. Workflow

### 10.1 Plan Commit

Before starting implementation, commit this plan file to the plans directory:

```bash
cd /workspace/abc_parse
cp /home/claude/.claude/plans/velvety-dancing-nest.md plans/18.treesitter-grammar.md
git add plans/18.treesitter-grammar.md
git commit -m "Add TreeSitter grammar implementation plan"
```

### 10.2 Worktree Setup

```bash
cd /workspace/abc_parse
git worktree add ../worktrees/treesitter-grammar -b feature/treesitter-grammar
cd /workspace/worktrees/treesitter-grammar
```

### 10.3 Phase Execution

For each phase:
1. Implement in worktree
2. Run `npm run build` from repo root
3. Run `npm run test` to verify
4. Code review agent on completed phase
5. Address feedback
6. Commit with descriptive message

### 10.4 Parallel Phase Code Reviews

When Phases 2B, 3, and 4 run in parallel (via sub-agents), each sub-agent must:
1. Complete its implementation
2. Request code review from the code-review agent
3. Address all code review feedback
4. Only then mark the phase as complete

The code reviews for parallel phases MUST also run in parallel. Each sub-agent is responsible for:
- Requesting its own code review
- Iterating on feedback until all issues are resolved
- Reporting completion only after passing code review

This ensures that all parallel work streams include quality checks without creating a sequential bottleneck.

### 10.5 Verification

Final verification checklist:
- [ ] `npm run build` succeeds from abc_parse root
- [ ] `npm run test` passes all tests
- [ ] All 63 example_scores parse without error
- [ ] Property-based tests pass with 10000 iterations
- [ ] CSNode comparison shows identical structure for all test inputs

---

## Critical Files

### To Create
- `tree-sitter-abc/package.json`
- `tree-sitter-abc/binding.gyp`
- `tree-sitter-abc/grammar.js`
- `tree-sitter-abc/src/scanner.h`
- `tree-sitter-abc/src/scanner.c`
- `tree-sitter-abc/bindings/node/binding.cc`
- `parse/comparison/CSNode.ts`
- `parse/comparison/exprToCS.ts`
- `parse/comparison/tsToCS.ts`
- `parse/comparison/compare.ts`
- `parse/tests/treesitter/generators.ts` - NEW ABC-specific generators
- `parse/tests/treesitter/helpers.ts` - Test helpers (parseWithBoth, etc.)
- `parse/tests/treesitter/comparison.spec.ts`
- `parse/tests/treesitter/pbt_treesitter.spec.ts`

### To Modify
- `package.json` (root) - add tree-sitter-abc to workspaces array, update build/test scripts

### Reference Files
- `parse/parsers/scan2.ts` - scanner architecture, TT enum (84 tokens)
- `parse/parsers/scan_tunebody.ts` - sub-scanner functions
- `parse/types/Expr2.ts` - all expression types for grammar rules
- `abct/tests/generators.ts` - example fast-check patterns (for ABCT, not ABC)
