# ABCt v2 Scanner Update

## Table of Contents

1. [Goal](#goal)
2. [Context](#context)
3. [Design Decisions](#design-decisions)
4. [Phase 1: Token Type Updates](#phase-1-token-type-updates)
5. [Phase 2: Operator Scanning Updates](#phase-2-operator-scanning-updates)
6. [Phase 3: Keyword Table Extension](#phase-3-keyword-table-extension)
7. [Phase 4: collectInvalid Update](#phase-4-collectinvalid-update)
8. [Phase 5: ABC Literal Primitive](#phase-5-abc-literal-primitive)
9. [Phase 6: Generator Updates](#phase-6-generator-updates)
10. [Phase 7: Example-Based Tests](#phase-7-example-based-tests)
11. [Phase 8: Property-Based Tests](#phase-8-property-based-tests)
12. [Phase 9: Round-Trip Verification](#phase-9-round-trip-verification)
13. [Implementation Checklist](#implementation-checklist)

---

## Goal

We are updating the ABCt scanner to support the v2 language surface syntax. The v1 scanner's architecture (context class, primitive composition, ABC fence handling) is reusable as-is for most changes. The bulk of the work is confined to token type definitions, the keyword lookup table, and the operator scanning function. One structural addition is required: a new `abcLiteral()` primitive function for scanning inline ABC literals delimited by single backticks.

---

## Context

### Source files

All files live in `abct/src/scanner/`:

- `types.ts` -- `AbctTT` enum and `Token` class
- `primitives.ts` -- `identifier()`, `number()`, `string()`, `abcFence()`, `operator()`, `collectInvalid()`
- `context.ts` -- `AbctCtx` class
- `scanner.ts` -- `scan()`, `scanProgram()`, `scanToken()`
- `whitespace.ts` -- `WS()`, `EOL()`, `comment()`
- `utils.ts` -- `isAtEnd()`, `advance()`, `peek()`, `matchPattern()`, etc.

### Test files

Tests live in `abct/tests/scanner/`:

- `primitives.spec.ts` -- example-based tests for each primitive function
- `roundtrip.spec.ts` -- property-based round-trip tests (source == concatenation of lexemes)
- `whitespace.spec.ts` -- whitespace/comment tests
- `generators.ts` -- fast-check arbitraries for all token types

### What we are adding (from the v2 language design, plan 18)

The v2 surface syntax introduces:
- 12 new keywords: `fn`, `match`, `over`, `let`, `if`, `then`, `else`, `topdown`, `bottomup`, `oncetd`, `alltd`, `load`
- 1 new two-character operator: `=>` (match arm arrow)
- 2 new single-character operators: `{` and `}` (record literals, match bodies)
- Inline ABC literals delimited by single backticks (`` `CEG A2 B2` ``)

### What we are removing

- `PIPE_EQ` (`|=`) -- the v2 design eliminates the dual-pipe model; `over` replaces `|=`

### Disambiguation note on `|capture|`

The Zig-style capture syntax `|c|` uses the same `|` character as the pipe operator. The scanner emits `PIPE` tokens in both cases. The parser resolves the ambiguity because captures only appear in a specific syntactic position (immediately after a tag name inside a match arm).

---

## Design Decisions

### Hyphenated names use underscores

Plan 18 uses hyphenated names for built-in functions (`add-voice`, `select-top`, `to-rest`, etc.). Because the scanner already treats `-` as the `MINUS` operator, allowing hyphens in identifiers would create ambiguity with arithmetic expressions like `a-b`. All built-in names use underscores instead: `add_voice`, `select_top`, `to_rest`, `set_rhythm`, `unwrap_single`, etc. No scanner change is needed for this; the existing identifier regex (`/[a-zA-Z_][a-zA-Z0-9_]*/`) already supports underscored names.

### Predicates do not use trailing `?`

Plan 18 uses a trailing `?` for predicate names (`is-rest?`, `is-note?`, `is-chord?`). We are dropping the `?` convention because it would require extending the identifier regex or adding a new operator token for a purely cosmetic benefit. Predicate names become `is_rest`, `is_note`, `is_chord`.

### `X:1` is tokenized as three separate tokens

The tune selector `X:1` is not a single token. The scanner emits `IDENTIFIER("X")`, `COLON`, `NUMBER("1")`. The parser reassembles these into a tune selector expression in the appropriate syntactic position.

### `insert` is a regular identifier

The `insert` verb from plan 18 is not a keyword. It is a regular identifier resolved at the semantic level (like `transpose`, `remove`, `add_voice`, etc.). Only control-flow and structural syntax constructs are keywords.

### Standalone `!` is not addressed

The `!` character is in the `collectInvalid` stop-set (for the `!=` two-character operator), but there is no single-character operator entry for it. A bare `!` in the source would currently be silently skipped by `scanToken`. Because the v2 design uses the `not` keyword for logical negation and `!=` for inequality (both already handled), standalone `!` has no defined meaning. We leave this as-is.

---

## Phase 1: Token Type Updates

### 1.1 Location

`abct/src/scanner/types.ts`

### 1.2 Changes to the AbctTT enum

Remove:

```typescript
PIPE_EQ, // |=
```

Add under a new "Keywords" section (after the existing `FILTER` entry):

```typescript
FN,       // fn
MATCH,    // match
OVER,     // over
LET,      // let
IF,       // if
THEN,     // then
ELSE,     // else
TOPDOWN,  // topdown
BOTTOMUP, // bottomup
ONCETD,   // oncetd
ALLTD,    // alltd
LOAD,     // load
```

Add under the "Operators" section:

```typescript
ARROW,    // =>
LBRACE,   // {
RBRACE,   // }
```

Add under a new "Literals" section (after the existing `STRING` entry):

```typescript
ABC_LITERAL_OPEN,    // opening single backtick
ABC_LITERAL_CONTENT, // the ABC text between backticks
ABC_LITERAL_CLOSE,   // closing single backtick
```

---

## Phase 2: Operator Scanning Updates

### 2.1 Location

`abct/src/scanner/primitives.ts`, function `operator()`

### 2.2 Remove PIPE_EQ

Delete the `|=` two-character operator check:

```typescript
// DELETE THIS:
if (ctx.test("|=")) {
  advance(ctx, 2);
  ctx.push(AbctTT.PIPE_EQ);
  return true;
}
```

### 2.3 Add ARROW

Add a new two-character operator check for `=>`. Because `=` and `>` are already single-character operators, the two-character check must come before them in the precedence order (this is already the case in the existing scanner architecture -- two-character checks run before single-character checks). Place it at the end of the existing two-character block, after the `!=` check:

```typescript
if (ctx.test("=>")) {
  advance(ctx, 2);
  ctx.push(AbctTT.ARROW);
  return true;
}
```

The ordering among two-character operators does not matter for correctness because `ctx.test()` checks both characters of the pattern. However, for readability, the final order should be: `>=`, `<=`, `==`, `!=`, `=>`.

### 2.4 Add LBRACE and RBRACE

Add `{` and `}` to the single-character operator array:

```typescript
["{", AbctTT.LBRACE],
["}", AbctTT.RBRACE],
```

---

## Phase 3: Keyword Table Extension

### 3.1 Location

`abct/src/scanner/primitives.ts`, function `identifier()`

### 3.2 Changes

Extend the `switch` statement in `identifier()` with all 12 new keywords:

```typescript
case "fn":       ctx.push(AbctTT.FN); break;
case "match":    ctx.push(AbctTT.MATCH); break;
case "over":     ctx.push(AbctTT.OVER); break;
case "let":      ctx.push(AbctTT.LET); break;
case "if":       ctx.push(AbctTT.IF); break;
case "then":     ctx.push(AbctTT.THEN); break;
case "else":     ctx.push(AbctTT.ELSE); break;
case "topdown":  ctx.push(AbctTT.TOPDOWN); break;
case "bottomup": ctx.push(AbctTT.BOTTOMUP); break;
case "oncetd":   ctx.push(AbctTT.ONCETD); break;
case "alltd":    ctx.push(AbctTT.ALLTD); break;
case "load":     ctx.push(AbctTT.LOAD); break;
```

---

## Phase 4: collectInvalid Update

### 4.1 Location

`abct/src/scanner/primitives.ts`, function `collectInvalid()`

### 4.2 Rationale

The `collectInvalid()` function advances through characters that do not match any scanner primitive. It stops when it encounters a character that would be a valid start of a recognized token. Because we are adding `{`, `}` as operators and the single backtick as a literal delimiter, all three characters must be added to the stop-set regex.

### 4.3 Changes

Update the regex in the `while` condition from:

```typescript
!ctx.test(/[\s"<\[(@|+=\-:>!.,]/)
```

to:

```typescript
!ctx.test(/[\s"<\[(@|+=\-:>!.,{}`]/)
```

(Adding `{`, `}`, and `` ` `` to the character class.)

---

## Phase 5: ABC Literal Primitive

### 5.1 Rationale

Plan 18 defines inline ABC literals using single backticks: `` `CEG A2 B2` ``. The existing `abcFence()` function handles the multi-line triple-backtick form (` ``` `). A new primitive function is needed for the single-backtick form. We follow the same open/content/close pattern that the triple-backtick fence uses, which allows the parser to handle both forms uniformly.

### 5.2 Location

`abct/src/scanner/primitives.ts` -- add a new exported function `abcLiteral()`

### 5.3 Token emission

The function emits three tokens for a valid literal:
1. `ABC_LITERAL_OPEN` -- the opening single backtick (lexeme: `` ` ``)
2. `ABC_LITERAL_CONTENT` -- everything between the backticks (lexeme: the ABC text, which may be empty)
3. `ABC_LITERAL_CLOSE` -- the closing single backtick (lexeme: `` ` ``)

### 5.4 Algorithm

```
abcLiteral(ctx):
  if peek(ctx) != '`'
    return false
  if ctx.test("```")       // triple backtick is handled by abcFence, not here
    return false
  advance(ctx, 1)
  ctx.push(ABC_LITERAL_OPEN)
  ctx.start = ctx.current  // reset start for content token
  while not isAtEnd(ctx) and peek(ctx) != '`' and peek(ctx) != '\n'
    advance(ctx, 1)
  ctx.push(ABC_LITERAL_CONTENT)
  if not isAtEnd(ctx) and peek(ctx) == '`'
    ctx.start = ctx.current
    advance(ctx, 1)
    ctx.push(ABC_LITERAL_CLOSE)
  // if we hit end-of-input or newline without a closing backtick,
  // we emit no CLOSE token -- the parser will report an unclosed literal error
  return true
```

### 5.5 Integration into scanToken

In `abct/src/scanner/scanner.ts`, the `scanToken()` function calls primitives in order. The `abcLiteral()` call must come before `abcFence()` in the dispatch sequence so that the triple-backtick check inside `abcLiteral` can delegate to `abcFence`. Actually, because `abcLiteral` explicitly returns false when it sees triple-backtick, the ordering is: `abcFence` first (checks for `\`\`\``), then `abcLiteral` (checks for single `` ` `` and already guards against triple). Either order works, but placing `abcFence` first preserves the existing dispatch order and is clearer.

Add to `scanToken()` after the existing `abcFence(ctx)` call:

```typescript
if (abcLiteral(ctx)) return;
```

### 5.6 Newline handling

An inline ABC literal cannot span multiple lines. If the scanner hits a newline before finding the closing backtick, it stops and emits only the OPEN and CONTENT tokens (no CLOSE). The parser will report the error. This is consistent with how string literals are handled in the existing scanner.

---

## Phase 6: Generator Updates

### 6.1 Location

`abct/tests/scanner/generators.ts`

### 6.2 Pre-existing bugs being fixed

The existing `genKeyword` generator only includes `"and"`, `"or"`, `"not"` but omits `"filter"`, which the scanner recognizes as a keyword. Similarly, the existing `genIdentifier` filter only excludes `["and", "or", "not"]` but not `"filter"`, so it can currently produce the string `"filter"` which the scanner classifies as `FILTER`. Both of these are pre-existing bugs that this update corrects by using a complete keyword set.

Additionally, the existing keyword property test (lines 105-115 of `primitives.spec.ts`) uses a ternary that only maps to `AND`/`OR`/`NOT`. This test must be replaced by the new keyword property test in Phase 8 (which checks `type !== IDENTIFIER` rather than mapping to specific token types).

### 6.3 Changes

#### Update genKeyword

Replace the current `genKeyword` with the complete set of all 16 keywords:

```typescript
export const genKeyword: fc.Arbitrary<string> = fc.constantFrom(
  "and", "or", "not", "filter",
  "fn", "match", "over", "let", "if", "then", "else",
  "topdown", "bottomup", "oncetd", "alltd", "load"
);
```

#### Update genIdentifier filter

Replace the existing filter with a complete exclusion set:

```typescript
const KEYWORDS = new Set([
  "and", "or", "not", "filter",
  "fn", "match", "over", "let", "if", "then", "else",
  "topdown", "bottomup", "oncetd", "alltd", "load"
]);

export const genIdentifier: fc.Arbitrary<string> = fc
  .stringMatching(/^[a-zA-Z_][a-zA-Z0-9_]{0,15}$/)
  .filter((id) => !KEYWORDS.has(id) && id.length > 0);
```

#### Update genDoubleOp

Remove `"|="` and add `"=>"`:

```typescript
export const genDoubleOp: fc.Arbitrary<string> = fc.constantFrom(
  ">=", "<=", "==", "!=", "=>"
);
```

#### Update genSingleOp and genSafeSingleOp

Add `"{"` and `"}"` to both generators. The distinction between these two generators: `genSingleOp` contains only the characters recognized by the `operator()` function. `genSafeSingleOp` is the subset used in round-trip property tests; it includes all of `genSingleOp` plus `.` and `,` (which are also single-char operators but handled separately in the scanner). Braces are unambiguous single-character tokens that cannot form multi-character operators with adjacent characters, so they are safe for round-trip testing.

```typescript
export const genSingleOp: fc.Arbitrary<string> = fc.constantFrom(
  "|", "+", "=", "@", ":", "-", "(", ")", "[", "]", ">", "<", "{", "}"
);

export const genSafeSingleOp: fc.Arbitrary<string> = fc.constantFrom(
  "|", "+", "=", "@", ":", "-", "(", ")", "[", "]", ".", ",", "<", ">", "{", "}"
);
```

#### Add genAbcLiteral

Add a new generator for inline ABC literals:

```typescript
export const genAbcLiteral: fc.Arbitrary<string> = fc
  .stringMatching(/^[A-Ga-g0-9 |[\]]{0,20}$/)
  .map((content) => "`" + content + "`");
```

This produces strings like `` `CEG A2` `` that are valid ABC literal tokens. The content regex avoids newlines and backticks.

---

## Phase 7: Example-Based Tests

### 7.1 Location

`abct/tests/scanner/primitives.spec.ts`

### 7.2 New keyword tests

Add one test per new keyword verifying that the identifier scanner produces the correct token type. Follow the existing pattern (see the `"and"`, `"or"`, `"not"` tests):

```typescript
it("should scan 'fn' as FN keyword", () => {
  const { ctx } = createTestCtx("fn");
  identifier(ctx);
  expect(ctx.tokens[0].type).to.equal(AbctTT.FN);
});
// ... similarly for match, over, let, if, then, else,
//     topdown, bottomup, oncetd, alltd, load
```

Also add tests verifying that identifiers starting with a keyword prefix are not misrecognized:

```typescript
it("should scan 'function' as IDENTIFIER, not FN", () => {
  const { ctx } = createTestCtx("function");
  identifier(ctx);
  expect(ctx.tokens[0].type).to.equal(AbctTT.IDENTIFIER);
  expect(ctx.tokens[0].lexeme).to.equal("function");
});

it("should scan 'overture' as IDENTIFIER, not OVER", () => {
  const { ctx } = createTestCtx("overture");
  identifier(ctx);
  expect(ctx.tokens[0].type).to.equal(AbctTT.IDENTIFIER);
});

it("should scan 'loading' as IDENTIFIER, not LOAD", () => {
  const { ctx } = createTestCtx("loading");
  identifier(ctx);
  expect(ctx.tokens[0].type).to.equal(AbctTT.IDENTIFIER);
});

it("should scan 'iffy' as IDENTIFIER, not IF", () => {
  const { ctx } = createTestCtx("iffy");
  identifier(ctx);
  expect(ctx.tokens[0].type).to.equal(AbctTT.IDENTIFIER);
});

it("should scan 'letter' as IDENTIFIER, not LET", () => {
  const { ctx } = createTestCtx("letter");
  identifier(ctx);
  expect(ctx.tokens[0].type).to.equal(AbctTT.IDENTIFIER);
});
```

### 7.3 New operator tests

Add tests for the arrow and brace operators:

```typescript
it("should scan '=>' as ARROW", () => {
  const { ctx } = createTestCtx("=>");
  operator(ctx);
  expect(ctx.tokens[0].type).to.equal(AbctTT.ARROW);
  expect(ctx.tokens[0].lexeme).to.equal("=>");
});

it("should scan '{' as LBRACE", () => {
  const { ctx } = createTestCtx("{");
  operator(ctx);
  expect(ctx.tokens[0].type).to.equal(AbctTT.LBRACE);
});

it("should scan '}' as RBRACE", () => {
  const { ctx } = createTestCtx("}");
  operator(ctx);
  expect(ctx.tokens[0].type).to.equal(AbctTT.RBRACE);
});
```

### 7.4 Operator ambiguity edge cases

Add tests to `roundtrip.spec.ts` in the "operator ambiguity edge cases" section that verify the new operator interacts correctly with existing ones:

```typescript
{ input: "=>>=", expected: ["=>", ">="] },
{ input: "=>=", expected: ["=>", "="] },
{ input: "===>", expected: ["==", "=>"] },
{ input: "=>=>", expected: ["=>", "=>"] },
{ input: "{}", expected: ["{", "}"] },
{ input: "{=>}", expected: ["{", "=>", "}"] },
{ input: "!=>", expected: ["!=", ">"] },
```

### 7.5 ABC literal tests

Add tests for the new `abcLiteral()` primitive:

```typescript
it("should scan a simple ABC literal", () => {
  const { ctx } = createTestCtx("`CEG A2`");
  abcLiteral(ctx);
  expect(ctx.tokens).to.have.length(3);
  expect(ctx.tokens[0].type).to.equal(AbctTT.ABC_LITERAL_OPEN);
  expect(ctx.tokens[0].lexeme).to.equal("`");
  expect(ctx.tokens[1].type).to.equal(AbctTT.ABC_LITERAL_CONTENT);
  expect(ctx.tokens[1].lexeme).to.equal("CEG A2");
  expect(ctx.tokens[2].type).to.equal(AbctTT.ABC_LITERAL_CLOSE);
  expect(ctx.tokens[2].lexeme).to.equal("`");
});

it("should scan an empty ABC literal", () => {
  const { ctx } = createTestCtx("``");
  abcLiteral(ctx);
  expect(ctx.tokens).to.have.length(3);
  expect(ctx.tokens[1].type).to.equal(AbctTT.ABC_LITERAL_CONTENT);
  expect(ctx.tokens[1].lexeme).to.equal("");
});

it("should not consume a triple-backtick as ABC literal", () => {
  const { ctx } = createTestCtx("```abc");
  const result = abcLiteral(ctx);
  expect(result).to.be.false;
});

it("should handle unclosed ABC literal at end of input", () => {
  const { ctx } = createTestCtx("`CEG A2");
  abcLiteral(ctx);
  expect(ctx.tokens).to.have.length(2);
  expect(ctx.tokens[0].type).to.equal(AbctTT.ABC_LITERAL_OPEN);
  expect(ctx.tokens[1].type).to.equal(AbctTT.ABC_LITERAL_CONTENT);
  // no CLOSE token emitted
});

it("should stop at newline without closing", () => {
  const { ctx } = createTestCtx("`CEG\nA2`");
  abcLiteral(ctx);
  expect(ctx.tokens).to.have.length(2);
  expect(ctx.tokens[1].lexeme).to.equal("CEG");
});
```

### 7.6 Removed PIPE_EQ tests

Delete all tests that reference `AbctTT.PIPE_EQ` or the `"|="` lexeme. In `roundtrip.spec.ts`, remove the following ambiguity test cases: `"|=|"`, `"|=|="`, `"|=|=|"`.

---

## Phase 8: Property-Based Tests

### 8.1 Location

`abct/tests/scanner/primitives.spec.ts`

### 8.2 Dependency on Phase 6

The property tests in this phase use generators updated in Phase 6 (`genKeyword`, `genIdentifier`, `genDoubleOp`, `genSafeSingleOp`). Phase 6 must be completed before these tests are written.

### 8.3 Replace existing keyword property test

The existing keyword property test (lines 105-115 of `primitives.spec.ts`) uses a ternary that only maps to `AND`/`OR`/`NOT` and will fail with the updated `genKeyword`. Delete it and replace with the following:

### 8.4 Property: all generated keywords scan to their correct token type

This property verifies that every keyword the generator can produce is recognized as a keyword (not an identifier):

```typescript
it("property: all generated keywords scan to keyword tokens", () => {
  fc.assert(
    fc.property(genKeyword, (kw) => {
      const { ctx } = createTestCtx(kw);
      identifier(ctx);
      return ctx.tokens[0].type !== AbctTT.IDENTIFIER;
    }),
    { numRuns: 1000 }
  );
});
```

### 8.5 Property: generated identifiers are never keywords

This property verifies that the `genIdentifier` filter correctly excludes all keywords:

```typescript
it("property: generated identifiers never produce keyword tokens", () => {
  fc.assert(
    fc.property(genIdentifier, (id) => {
      const { ctx } = createTestCtx(id);
      identifier(ctx);
      return ctx.tokens[0].type === AbctTT.IDENTIFIER;
    }),
    { numRuns: 5000 }
  );
});
```

### 8.6 Property: arrow operator does not consume following characters

This property ensures that `=>` followed by any valid token start does not over-consume:

```typescript
it("property: '=>' followed by any token scans as ARROW + remainder", () => {
  fc.assert(
    fc.property(genAnyToken, (suffix) => {
      const source = "=>" + suffix;
      const { tokens } = scanSource(source);
      return tokens[0].type === AbctTT.ARROW && tokens[0].lexeme === "=>";
    }),
    { numRuns: 5000 }
  );
});
```

### 8.7 Property: braces in token sequences preserve round-trip

This property verifies that the addition of brace tokens does not break the round-trip invariant (concatenation of lexemes == source):

```typescript
const genTokenWithBraces: fc.Arbitrary<string> = fc.oneof(
  genIdentifier, genKeyword, genNumber, genString,
  genSafeSingleOp, genDoubleOp, genWS
);

it("property: token sequences with braces round-trip", () => {
  fc.assert(
    fc.property(
      fc.array(genTokenWithBraces, { minLength: 1, maxLength: 15 }),
      (parts) => {
        const source = parts.join("");
        const reconstructed = reconstructSource(source);
        return reconstructed === source;
      }
    ),
    { numRuns: 5000 }
  );
});
```

### 8.8 Property: ABC literals round-trip

This property verifies that inline ABC literals scan and reconstruct correctly:

```typescript
it("property: ABC literals round-trip", () => {
  fc.assert(
    fc.property(genAbcLiteral, (literal) => {
      const { tokens } = scanSource(literal);
      const reconstructed = tokens.map(t => t.lexeme).join("");
      return reconstructed === literal;
    }),
    { numRuns: 5000 }
  );
});
```

---

## Phase 9: Round-Trip Verification

### 9.1 Location

`abct/tests/scanner/roundtrip.spec.ts`

### 9.2 Rationale

The existing round-trip test suite uses the `genTokenSequence` generator, which composes `genAnyToken`. Because `genAnyToken` uses `genSafeSingleOp` and `genDoubleOp`, and both generators are updated in Phase 6, the existing round-trip property tests will automatically exercise the new tokens.

### 9.3 Update genAnyToken

Add `genAbcLiteral` to the `genAnyToken` composite generator so that round-trip tests also exercise inline ABC literals:

```typescript
export const genAnyToken = fc.oneof(
  genIdentifier, genKeyword, genNumber, genString,
  genSafeSingleOp, genDoubleOp, genAbcLiteral, genWS
);
```

### 9.4 Verification

After all changes are applied, run the existing round-trip property tests to confirm:
- The new operators (`=>`, `{`, `}`) round-trip correctly.
- The new ABC literal tokens round-trip correctly.
- The removed operator (`|=`) no longer appears in generated test cases.
- No new ambiguity interactions break the concatenation invariant.

---

## Implementation Checklist

- [ ] Phase 1: Remove `PIPE_EQ` from `AbctTT`, add `ARROW`, `LBRACE`, `RBRACE`, 12 keyword entries, and 3 ABC literal token types
- [ ] Phase 2: In `operator()`, remove the `|=` check, add the `=>` check after `!=`, add `{` and `}` to the single-character array
- [ ] Phase 3: In `identifier()`, extend the switch statement with 12 new keyword cases
- [ ] Phase 4: In `collectInvalid()`, add `{}` and `` ` `` to the stop-set regex
- [ ] Phase 5: Add `abcLiteral()` primitive in `primitives.ts`, integrate into `scanToken()` after `abcFence()`
- [ ] Phase 6: In `generators.ts`, update `genKeyword` (fix pre-existing `filter` omission), `genIdentifier` filter (fix pre-existing `filter` omission), `genDoubleOp`, `genSingleOp`, `genSafeSingleOp`, and add `genAbcLiteral`
- [ ] Phase 7: Add example-based tests for all new keywords, new operators, keyword-prefix identifiers, operator ambiguity edge cases (including `!=>`) , ABC literal happy/error paths. Remove PIPE_EQ tests.
- [ ] Phase 8: Replace the existing broken keyword property test. Add property-based tests for keyword correctness, identifier exclusion, arrow non-overconsumption, brace round-trip, and ABC literal round-trip.
- [ ] Phase 9: Add `genAbcLiteral` to `genAnyToken`, run the full test suite (`npm run test`) and verify all round-trip properties pass
- Final verification: `npm run build` and `npm run test` both pass.
- Call the code review agent. Address any feedback.
- Commit once the build passes and all tests pass.

